---
title: "Data - Adult Dataset"
author: "Pablo Morala"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

####################################
# 1 - Load all needed packages and functions
####################################
library(nn2poly)
library(nn2poly.tools)
library(keras)
library(tensorflow)
library(cowplot)
library(ggplot2)

IEEE_width_inches <- 3.5
my_width <- IEEE_width_inches

my_seed <- 42
set.seed(my_seed)
tensorflow::tf$random$set_seed(my_seed) # Needed to have reproducible results with keras
```




# Data preparation

```{r data-loading}
# Load the data
data_raw <- read.table("real-datasets/adult/adult.data", sep = ",")

head(data_raw)
# names(data_raw)
# summary(data_raw)


```


```{r data-preparation}
require(tidyr)
require(dplyr)

# change all character columns to factors
df <- data_raw %>%
  mutate_if(sapply(data_raw, is.character), as.factor)

head(df)
summary(df)

df %>% mutate(value = 1)  %>% spread(V2, value,  fill = 0 ) 



# Define dimension p (number of predictor variables)
p <- dim(data_raw)[2] - 1

# Define objective classes
n_class <- max(data_raw[,(p+1)])

# Move objective classes from (1:7) to (0:6), needed for tensoflow
data_raw[,(p+1)] <- data_raw[,(p+1)] - 1
```




```{r data-scaling}
# Scale the data in the desired interval and separate train and test
# Only the predictor variables are scaled, not the response as those will be
# the different classes. Note that variable 55 is the response in this case.
scale_method <- "-1,1"
data_x_scaled <- scale_data(data_raw[,-(p+1)], scale_method)
data_scaled <- cbind(data_x_scaled, data_raw[,(p+1)])

# Now, rejoin with Y and divide in train and test
aux <- divide_train_test(data_scaled, train_proportion = 0.80)
train <- aux$train
test <- aux$test


# Divide again in x and y to use in tensorflow and turn into matrix form
train_x <- as.matrix(train[,-(p+1)])
train_y <- as.matrix(train[,(p+1)])

test_x <- as.matrix(test[,-(p+1)])
test_y <- as.matrix(test[,(p+1)])
```

# Neural Network training

First, we determine the model by defining its hyperparameters and structure:

```{r hyperparams}
# keras hyperparameters 
  my_loss <- loss_sparse_categorical_crossentropy(from_logits = TRUE)
  my_metrics <- "accuracy"
  my_optimizer <- optimizer_adam()
  my_epochs <- 1000
  my_validation_split <- 0.1
  my_verbose <- 1
  my_batch_size <- 100

  # Custom parameters using nn2poly.tools:
  af_string_list <- list("tanh", "tanh", "tanh", "linear")
  h_neurons_vector <- c(100, 100, 100, n_class) 
  my_max_norm <- list("l1_norm",1)
  
```

Then we build, compile and train.

```{r nn-build}
nn <- build_keras_model(p,
    af_string_list,
    h_neurons_vector,
    my_max_norm)

compile(nn,
        loss = my_loss,
        optimizer = my_optimizer,
        metrics = my_metrics)

history <- fit(nn,
               train_x,
               train_y,
               verbose = my_verbose,
               epochs = my_epochs,
               validation_split = my_validation_split,
               batch_size = my_batch_size
)
```


```{r plot-history}
plot(history)
```

The results from the previous model needs to be transformed into probabilities using softmax and then assigining a class based on the highest probability: 

```{r probability-model}
probability_model <- keras_model_sequential() %>%
  nn() %>%
  layer_activation_softmax() %>%
  layer_lambda(k_argmax)
```

We can then get a confusion matrix to see the results:

```{r comparison-y-nn}
# Obtain the predicted values with the NN to compare them
prediction_NN <- predict(probability_model, test_x)

# Create a confusion matrix
cm <- caret::confusionMatrix(as.factor(prediction_NN), as.factor(test_y))
cm

```


## Obtaining the polynomial regression

After the NN has been trained, using any chosen method by the user, the parameters have to be extracted and reshaped, if needed, to match the expected input of the function `nn2poly_algorithm()`. This input consists in the following objects:

* `weights_list` A list of matrices with a weight matrix at each layer. The weights matrices should be of dimension ((1+input) * output) where the first row corresponds to the bias vector, and the rest of the rows correspond to each of the ordered vector weights associated to each input.
* `af_string_list` A list of strings with the names of the activation functions at each layer.
* `q_taylor_vector` A vector of integers containing the order of the Taylor expansion performed at each layer. If the output layer has a linear activation function, then the last value should be 1.

Following the example of the NN that we created previously, we need to extract its weights and biases and reshape them. Particularly, the `keras` framework by default separates kernel weights matrices of dimension (input * output) and bias vectors (1 * output), so we need to add the bias as the first row of a matrix ((1+input) * output).


```{r}
keras_weights <- keras::get_weights(nn)

n <- length(keras_weights)

nn_weights <- keras_weights[1:(n-2)]

nn_weights[[n-1]] <- rbind(keras_weights[[n]], keras_weights[[n-1]])


```


And finally the order of the Taylor approximation that we are going to choose is 3 at each hidden layer.

```{r}
q_taylor_vector <- c(8, 8, 8, 1) 
```

When the input is in the desired shape, the method can be applied finally:

```{r}
all_layers_coeffs <- nn2poly_algorithm(
  weights_list = nn_weights,
  af_string_list = af_string_list,
  q_taylor_vector = q_taylor_vector,
  forced_max_Q = 2
)

final_poly <- all_layers_coeffs[[length(all_layers_coeffs)]]
```

---
TODO: Poner opción para que la función no guarde los coeficientes intermedios
---

We can have a glimpse at how the coefficients of the pollynomial regression (PR) are stored.
```{r}
final_poly$values[,1:7]
```


## Visualising the results

Para predecir, tenemos que usar los polinomios para sacar los valores y después softmax y argmax

```{r}
# Obtain the predicted values for the test data with our Polynomial Regression
prediction_poly_matrix <- eval_poly(x = test_x, poly = final_poly)

a <- tf$math$softmax(prediction_poly_matrix)
b <- as.integer(tf$math$argmax(a))

summary(as.factor(b))

cm <- caret::confusionMatrix(as.factor(prediction_NN), as.factor(b))
cm
```


Vemos que funciona regular, pero es por no tener constraints?

Los plots de la expansion de Taylor son iguales aunque tengamos varias salidas,
lo unico es que la capa de salida no se está haciendo Taylor, pero no pasa nada
porque es la softmax. 
En este caso parece que en la segunda capa es dond ehay mas fallo.

```{r potentials}
plot_taylor_and_synpatic_potentials(data = train,
                                    weights_list = nn_weights,
                                    af_string_list = af_string_list,
                                    q_taylor_vector = q_taylor_vector)
```










