---
title: "Data - Covertype Dataset"
author: "Pablo Morala"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

####################################
# 1 - Load all needed packages and functions
####################################
library(nn2poly)
library(nn2poly.tools)
library(keras)
library(tensorflow)
library(cowplot)
library(ggplot2)

IEEE_width_inches <- 3.5
my_width <- IEEE_width_inches

my_seed <- 42
set.seed(my_seed)
tensorflow::tf$random$set_seed(my_seed) # Needed to have reproducible results with keras
```


This document contains an example of use for NN2Poly applied to the Covertype Dataset (https://archive.ics.uci.edu/ml/datasets/covertype), which consists of a cartographic classification problem into 7 types of forest.

- The variables are only cartographic and do not use any kind of remotely sensed data. 
- Data not scaled
- And contains binary columns (0 or 1) for qualitative variables
  - 12 first, quantitative
  - 13 to 54, binary variables
  - 55, response variable Y

# Data preparation

```{r data-loading}
# Load the data
data_raw <- read.table("real-datasets/covertype/covtype.data.gz", sep = ",")

head(data_raw)
# names(data_raw)
# summary(data_raw)

# Define dimension p (number of predictor variables)
p <- dim(data_raw)[2] - 1

# Define objective classes
n_class <- max(data_raw[,(p+1)])

# Move objective classes from (1:7) to (0:6), needed for tensoflow
data_raw[,(p+1)] <- data_raw[,(p+1)] - 1
```

```{r data-scaling}
# Scale the data in the desired interval and separate train and test
# Only the predictor variables are scaled, not the response as those will be
# the different classes. Note that variable 55 is the response in this case.
scale_method <- "-1,1"
data_x_scaled <- scale_data(data_raw[,-(p+1)], scale_method)
data_scaled <- cbind(data_x_scaled, data_raw[,(p+1)])

# Now, rejoin with Y and divide in train and test
aux <- divide_train_test(data_scaled, train_proportion = 0.75)
train <- aux$train
test <- aux$test


# Divide again in x and y to use in tensorflow and turn into matrix form
train_x <- as.matrix(train[,-(p+1)])
train_y <- as.matrix(train[,(p+1)])

test_x <- as.matrix(test[,-(p+1)])
test_y <- as.matrix(test[,(p+1)])
```

# Neural Network training

First, we determine the model by defining its hyperparameters and structure:

```{r hyperparams}
# keras hyperparameters 
  my_loss <- loss_sparse_categorical_crossentropy(from_logits = TRUE)
  my_metrics <- "accuracy"
  my_optimizer <- optimizer_adam()
  my_epochs <- 2000
  my_validation_split <- 0.2
  my_verbose <- 0
  my_batch_size <- 100

  # Custom parameters using nn2poly.tools:
  af_string_list <- list("tanh", "tanh", "tanh", "linear")
  h_neurons_vector <- c(100, 100, 100, n_class) 
  my_max_norm <- list("l1_norm",1)
  
```

Then we build, compile and train.

```{r nn-build}
nn <- build_keras_model(p,
    af_string_list,
    h_neurons_vector,
    my_max_norm)

compile(nn,
        loss = my_loss,
        optimizer = my_optimizer,
        metrics = my_metrics)

history <- fit(nn,
               train_x,
               train_y,
               verbose = my_verbose,
               epochs = my_epochs,
               validation_split = my_validation_split,
               batch_size = my_batch_size
)
```


```{r plot-history}
plot(history)
```

The results from the previous model needs to be transformed into probabilities using softmax and then assigining a class based on the highest probability: 

```{r probability-model}
probability_model <- keras_model_sequential() %>%
  nn() %>%
  layer_activation_softmax() %>%
  layer_lambda(k_argmax)
```

We can then get a confusion matrix to see the results:

```{r comparison-y-nn}
# Obtain the predicted values with the NN to compare them
prediction_NN <- predict(probability_model, test_x)

# Create a confusion matrix
cm <- caret::confusionMatrix(as.factor(prediction_NN), as.factor(test_y))
cm

```


